# Paimon's Sticker Stash
![Paimon: Ship out!](public/stickers/set_1/Icon_Emoji_Paimon's_Paintings_01_Paimon_2.webp)

[Paimon's Sticker Stash](https://paimons-sticker-stash.netlify.app) is basically a consolidated stash of stickers from the hit video game *Genshin Impact*, scraped from the [Fandom Wiki](https://genshin-impact.fandom.com/wiki/Genshin_Impact_Wiki). It supports text search for sticker titles, and groups stickers by Paimon's Painting sets and by character.

The Sticker Stash also supports favouriting stickers for quick access (stored in browser LocalStorage), and since stickers are just images, you can copy them to use in messaging software like Discord or whatever. It also (theoretically) supports a [public API](https://paimons-sticker-stash.netlify.app/api) to query sticker data. Or you can just steal `stickers.db` and the `images/stickers` folder from this repository.

## Tech Stack
![Jean: Time for Work](public/stickers/set_31/Icon_Emoji_Paimon's_Paintings_31_Jean_1.webp)

- **NextJS** w/ Typescript for frontend & API routes
  - **Prisma ORM** for reading data from SQLite database
- **Python** scraper for backend w/ beautifulsoup
- **SQLite** for database storage
- **Github Actions** to automate scraper runs
- **Netlify** for autodeployment

## API
![Kiara: Right Away!](public/stickers/set_24/Icon_Emoji_Paimon's_Paintings_24_Kirara_3.webp)

In theory, there should be a public API for read-only access to the database (the only writing is either manual or via autoupdate). Autogenerated documentation is at https://paimons-sticker-stash.netlify.app/api.

I haven't extensively tested this, nor fully ensured that `Access-Control-Allow-Origin: *` is set so if it doesn't work then it may be cooked. Add an issue or something, or just take this repo's scraper and build a database of your own.

The API should allow for things like get all characters/stickers/sets, get specific sticker/character/set by id, and some other optional parameters like a search query. Anything that can be done in the frontend should be doable with the API (assuming it works in the first place).

## Setup
![Aino: Unknown](public/stickers/set_44/Icon_Emoji_Paimon's_Paintings_44_Aino_4.webp)

This repo is a monorepo which contains the Python scraper and the NextJS files for Paimon's Sticker Stash.

The Python scraper is triggered on a schedule via GitHub Actions. The job will:
1. Run the scraper, which will:
   1. Look for any new stickers in the latest *numerical* set (it seems like named sets are fairly rare, and detecting those would require scraping the general Wiki page which requires reprocessing unnecessary data)
   2. Update `stickers.db` database and download the images to `images/stickers/set_{name}/{image_name}`
2. Commit and push changes to the repo, triggering a Netlify redeployment.


The NextJS frontend and API routes works with `stickers.db` via Prisma ORM to read data from the database.

### Frontend (NextJS)
If for any reason you want to run Paimon's Sticker Stash locally on your machine, simply:
1. Clone the repository.
2. Install dependencies with `npm install`
3. Generate Prisma ORM client:
   - Technically I should've hardcoded it to use `stickers.db` but whatever. Create a `.env` file in the project root with `DATABASE_URL="file:./stickers.db"`.
   - Run `npx prisma generate` to generate the client.
4. `npm run dev` and it should appear on `localhost:3000`.

### Python Scraper
If for any reason you want to run the... *questionably*-built Python scraper held together by hopes and dreams, the files are all located under `scraper`. I will rewrite it once it breaks, but it works at the moment so there's not much need to worry. As edge cases appear I will update to handle them, but since the Fandom Wiki appears to be consistently structured, it should be fine for a while.
1. Clone the repository.
2. Install requirements `pip install -r ./scraper/requirements.txt` or something like that
3. Should be able to run the scraper now. There should be a bunch of logs in the console to tell you what it's doing. You can change what the scrape does by editing `scraper.py`'s `main` block. Some useful functions (e.g., scrape specific set) are provided already.

## A Word of Warning
![Wanderer: Sobs](public/stickers/set_20/Icon_Emoji_Paimon's_Paintings_20_Wanderer_2.webp)

As a short personal project of mine, there are a few sleeping issues which might come up to be a problem in the future for people who choose to use Paimon's Sticker Stash.

### Database Instability
What I have made is not perfect, especially on the database side. There were some questionable decisions made in designing the schema for a database which unforunately has a partial loop of foreign key relations. 

This means that if the scraper messes up, fixing/deleting things is going to be... not fun to say the least (that's why it's version controlled on Github! Praise the version control (until it gets too large and I need to learn git LFS)).

In any case, I _believe_ the better solution is to make extra tables to manage the relation between a sticker set or character's main sticker to avoid those circular references, but at this point, it works at the moment so not much need to change it.

Now, the issue comes from the fact that favourite stickers are stored in LocalStorage as **an array of sticker IDs**, meaning if I ever need to reconstruct the database from scratch (the worst-case scenario) **the sticker with ID X may be different**, rendering people's favourite lists full of junk.

### Image Inconsistency
Another issue is that images stored internally are not all of the same size. All images are scraped as their original size from the Fandom Wiki, and I did not do any extra processing nor conversion.

Notably, every sticker in **Set Fortuitous Encounter the Coral Sea** is of much lower resolution than the other stickers, as it is a collaboration with Xiaohongshu (some sort of social media app in China or something) which apparently uses smaller emoji sizes.

The frontend mitigates this issue with `next/image` image resizing, but if you are using the images directly, you should keep the variable image sizes in mind.
